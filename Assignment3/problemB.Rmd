---
output: html_document
editor_options: 
  chunk_output_type: console
chunk_output_type: console
---

# Hierarchical models: Random variance

## Question 1
We are given the following equation:

\begin{equation}
clo_{i,j,k} = \mu +\beta(sex_i)+u_i+\varepsilon_{ijk};\hspace{0.5cm} u_i\sim N(0,\sigma^2_u), \hspace{0.5cm} \varepsilon_{ijk}\sim N(0,\sigma^2)
\end{equation}

where $clo_{i,j,k}$ is the clothing insulation level of the subject i on day $j$ and $k$ refer to the observation number within the day. $u_i$ is a subject specific effect/preference. 

As this is a linear (gaussian) mixed effect model that has been written on the standard form (Definition 5.3 [1]), we can estimate the parameters by maximizing the marginal log-likelihood (equation (5.47) and (5.48) [1]):

\begin{equation}
l_M(y,\beta,\sigma_u,\sigma)\propto -\frac{1}{2}\left[\log|V|+(y-X\beta)^TV^{-1}(y-X\beta) \right]
\end{equation}


where $V= I\sigma^2+Z(I\sigma_u^2) Z^T$ and $U\sim N(0,I\sigma_u^2)$ and $\varepsilon \sim N(0, I\sigma^2)$.

Optimizing this problem is however not the most computationally most efficient method of estimating the likelihood, because the the covariance matrix $V$ has the same number of dimensions as we have observations. 
The inversion of this matrix is therefore very expensive to compute. We could also run into numerical issues leading to unstable estimations. 

We can however take advantage of the grouping structure of the covariance matrix, as it is is a block diagonal matrix:

\begin{equation}
\textbf{V}=\begin{bmatrix}\textbf{V}_1 & 0  & \cdots &0\\
0&\textbf{V}_2 & \cdots & 0 \\
 \vdots & \vdots & \ddots  &\vdots \\
 0 & 0 & \cdots & \textbf{V}_n\end{bmatrix}\label{eq:block_eq}
 \end{equation}

where $V_i$ corresponds to the observations belonging to subject $i$.

This means that instead of estimating the full covariance matrix all at once, we can separate it into smaller subproblems as subject 1 is independent of all the other subjects. This means we can simply estimate the probability of each of the subjects individually and multiply them together to get the likelihood. 

This will change our marginal log-likelihood function into:



\begin{equation}
l_M(y,\beta,\sigma_u,\sigma)\propto -\frac{1}{2}\sum_{i}^{N_{subjId}} \log |\sigma^2I_{n_i}+\sigma_u^2E_{{n_i},{n_i}} |+ (y_i-X_i\beta)^T\left[\sigma^2I_{n_i}+\sigma_u^2E_{{n_i},{n_i}}\right]^{-1}(y_i-X_i\beta)
\end{equation}

Where $i$ all the indices for each of the subjects, and where $n_i$ is the number of observations for each subject. $E_{n_i,n_i}$ is a matrix full of ones with the dimensions $n_i\times n_i$ and $I_{n_i}$ is the identity matrix of the same dimensions. This means that the covariance matrix for each subject is a full covariance matrix filled with $\sigma_u^2$ and with $\sigma_u^2+\sigma^2$ along the diagonal. 

We construct our $X_i$ matrix such that it has the first column consists of ones, estimating $\mu$, and the second column consists of ones if the subject is male otherwise it consists of zeros to estimate $\beta$. This means that $\mu$ denotes the intercept for females and $\mu+\beta$ denotes the intercept for male subjects.


```{r}
#setwd(dirname(rstudioapi::getSourceEditorContext()$path))
#rm(list=ls())
dat = read.csv(file="data/clothingFullAss03.csv")
dat$sex_male = as.numeric(as.factor(dat$sex))-1
dat$sex_female = abs(as.numeric(as.factor(dat$sex))-2)
library(lme4)
fit0 <- lmer(clo~sex+(1|subjId),data=dat,REML=FALSE)
```



```{r}
library(mvnfast)
pars=c("mu"=0.5,"sex_male"=-0.1,"sigma_s"=0.1,"sigma"=0.1)
# 
# kronecker(diag(3),matrix(c(1.1,1,1,1.1),ncol=2))
# 

# 
# X = cbind(1,rep(0,nrow(dat)))
# X[dat$sex=="male",2]=1
# library(Matrix)
# Z = Matrix(0,nrow=nrow(dat),ncol=length(unique(dat$subjId)),sparse=T)
# for(i in 1:nrow(dat)){
#   Z[i,unique(dat$subjId)==dat$subjId[i]]=1
# }
# m = diag(c(1,2))
# m[1,2]=m[2,1]=0.5
# m
# chol2inv(chol(m))
# solve(m)
# qr.solve(m)
# # slow
# #library(mvtnorm)


nll0<- function(pars){
  # ML estimation:
    mu = pars["mu"]
    sex_male = pars["sex_male"]
    beta=c(mu,sex_male)
    
    # y = dat$clo
    # Psi = diag(length(unique(dat$subjId)))*pars["sigma_s"]^2
    # Sigma = diag(nrow(dat))*pars["sigma"]^2
    # V = Z%*%Psi%*%t(Z)+Sigma
    # Faster dmvnorm from mvnfast
    # ll = dmvn(y,X%*%beta,V,log=T)
    
    ll=0
    
    for(i in unique(dat$subjId)){
      sex = dat$sex[dat$subjId==i][1]
      n = sum(dat$subjId==i)
      if(sex=="male"){
        x = cbind(1,rep(1,n))
      }else{
        x = cbind(1,rep(0,n))
      }
      sig = diag(n)*pars["sigma"]^2+pars["sigma_s"]^2
      y = dat$clo[dat$subjId==i]
      tmp = dmvn(y,x%*%beta,sig,log=T)
      ll = ll+tmp
    }
   
    return(-ll)
}

# Optimization
opt0<-nlminb(pars,nll0,lower=c(-10,-10,0.001,0.001),
       upper=c(10,10,10,10))
# estimated parameters
opt0$par
# lme4 estimates
fixef(fit0)
VarCorr(fit0)

# Difference in ll
logLik(fit0)+nll0(opt0$par)


library(numDeriv)
H = hessian(nll0,opt0$par)

m1  = cbind(c(fixef(fit0),sqrt(unlist(unlist(VarCorr(fit0)))),
              sigma(fit0)),opt0$par)

m2 = cbind(opt0$par+qnorm(0.025)*sqrt(diag(solve(H))),
           opt0$par+qnorm(0.975)*sqrt(diag(solve(H))))

m = cbind(m1,m2)
xtable::xtable(m,digits=4)

```

The maximum log-likelihood estimate of our model is 643.79. If we compare our estimate to the build-in functions to estimate the model, we get a difference of $8.8\cdot 10^{-9}$ between the two estimates, which also corresponds to the tolerance level of the used optimizer \texttt{nlminb}. 


\begin{table}[ht]
\centering
\begin{tabular}{|c|c|ccc|}
  \hline
 & \texttt{lme4} & estimate & 2.5\%  & 97.5\% \\ 
  \hline
$\mu$ & 0.5918 & 0.5918 & 0.5441 & 0.6394 \\ 
  $\beta$ & -0.0832 & -0.0832 & -0.1514 & -0.0151 \\ 
  $\sigma_u$ & 0.1167 & 0.1167 & 0.0921 & 0.1412 \\ 
  $\sigma$ & 0.0988 & 0.0988 & 0.0939 & 0.1038 \\ 
   \hline
\end{tabular}
\caption{This table shows the \texttt{lme4} estimate along with the estimate of our model. The 95\% Wald confidence interval estimated with our model is also shown.}\label{tab:q2_1}
\end{table}
Table \ref{tab:q2_1} shows that our estimated parameters correspond to the estimated parameters from the \texttt{lme4}-package. 



## Question 2
We are now given the model:


\begin{equation}
clo_{i,j,k} = \mu +\beta(sex_i)+u_i+v_{ij} +\varepsilon_{ijk};\hspace{0.5cm} u_i\sim N(0,\sigma^2_u),\hspace{0.5cm} v_{ij}\sim N(0,\sigma^2_v), \hspace{0.5cm} \varepsilon_{ijk}\sim N(0,\sigma^2)
\end{equation}

where $v_{ij}$ is the subject specific within a specific day effect (interaction term between subjects and day). 

Like before, we can write the marginal log-likelihood by estimating each of the subjects likelihood separately. Now, the covariance structure will just get even more complicated. 


\begin{equation}
l_M(y,\beta,\sigma_u,\sigma)\propto -\frac{1}{2}\sum_{i}^{N_{subjId}} \log |\Sigma^{(i)} |+ (y_i-X_i\beta)^T\left(\Sigma^{(i)}\right)^{-1}(y_i-X_i\beta)
\end{equation}

where $\Sigma^{(i)}$ is the covariance matrix for each subject (the block elements,$\textbf{V}_i$ of the full covariance matrix $\textbf{V}$ from equation \ref{eq:block_eq}).

The structure of $\Sigma^{(i)}$ can be written as:

\begin{equation}
\Sigma_{n,m}^{(i)} = \begin{cases}\sigma^2+\sigma_u^2+\sigma_v^2 & n=m  \\
\sigma_u^2+\sigma_v^2&\text{if } day_n = day_m \\
\sigma_u^2 & \text{if } day_n \neq day_m \
\end{cases}\label{eq:Sigma_i}
\end{equation}

where $n$ and $m$ correspond to the observation number for a specific subject. 


```{r}
fit1 <- lmer(clo~sex+(1|subjId)+(1|subjId:day),data=dat,REML=F)


nll1<- function(pars){
  # ML estimation:
    mu = pars["mu"]
    sex_male = pars["sex_male"]
    beta=c(mu,sex_male)
    
    # for(j in unique(dat$subjId)){
    #   tmp = c()
    #   n_days = unique(dat$day[dat$subjId==j])
    #   for(k in n_days){
    #     tmp = c(tmp,sum(dat$subjId==j&dat$day==k))
    #   }
    #   cat("\n","subject",j,"\t",paste("day",n_days,"n_obs",tmp))
    # }
    # 
    
    ll=0
    for(i in unique(dat$subjId)){
      df = dat[dat$subjId==i,]
      
      sex = df$sex[1]
      n = nrow(df)
      days = unique(df$day)
      
      
      if(sex=="male"){
        x = cbind(1,rep(1,n))
      }else{
        x = cbind(1,rep(0,n))
      }
      
      sig = diag(n)*pars["sigma"]^2+pars["sigma_s"]^2
      for(k in days){
        idx = which(df$day==k)
        sig[idx,idx] = sig[idx,idx]+pars["sigma_sub_day"]^2
      }
      sig
      y = df$clo
      tmp = dmvn(y,x%*%beta,sig,log=T)
      #tmp = dmvnorm(y,x%*%beta,sig,log=T)
      #print(tmp)
      ll = ll+tmp
    }
    
    return(-ll)
}


pars=c("mu"=0.5,"sex_male"=-0.1,"sigma_sub_day"=0.1,"sigma_s"=0.1,"sigma"=0.1)

nll1(pars)

opt1<-nlminb(pars,nll1,lower=c(-10,-10,0.001,0.001,0.001),
       upper=c(10,10,10,10,10))

logLik(fit1)+nll1(opt1$par)


H = hessian(nll1,opt1$par)



m1  = cbind(c(fixef(fit1),sqrt(unlist(unlist(VarCorr(fit1)))),
              sigma(fit1)),opt1$par)

m2 = cbind(opt1$par+qnorm(0.025)*sqrt(diag(solve(H))),
           opt1$par+qnorm(0.975)*sqrt(diag(solve(H))))

m = cbind(m1,m2)
xtable::xtable(m,digits=4)

```

The estimated log-likelihood of the model is 943.13 and the difference from the \texttt{lme4} estimate is $2.6\cdot 10^{-9}$. 

\begin{table}[ht]
\centering
\begin{tabular}{|c|c|ccc|}
  \hline
 & \texttt{lme4} & Estimate & 2.5\% & 97.5\% \\ 
  \hline
$\mu$ & 0.5924 & 0.5924 & 0.5446 & 0.6402 \\ 
  $\beta$ & -0.0844 & -0.0844 & -0.1528 & -0.0159 \\ 
  $\sigma_v$ & 0.0973 & 0.0973 & 0.0823 & 0.1123 \\ 
  $\sigma_u$ & 0.1039 & 0.1039 & 0.0759 & 0.1320 \\ 
 $\sigma$  & 0.0560 & 0.0560 & 0.0530 & 0.0590 \\ 
   \hline
\end{tabular}
\caption{This table show the parameter estimates of our model and \texttt{lme4}. We also show 95\% (Wald) confidence interval}\label{tab:q2_2}
\end{table}

Table \ref{tab:q2_2} shows the estimated parameters of our model. We see that most of the estimates are the same as in table \ref{tab:q2_1}, but the non-modeled variance has decreased and has been moved to $\sigma_v$.



```{r resit_fit1,fig.cap="This figure shows the residuals for males and females of the \\texttt{lme4} model. The colors indicate different subjects and the connected line shows the same day for a given subject.",fig.height=4}

library(ggplot2)
dat$resid = resid(fit1)
ggplot(dat,aes(x=tOut,y=resid,col=factor(subjId),group=subDay))+
  geom_point()+geom_line()+
  facet_wrap(~sex)+theme(legend.position = "none")+
  xlab("Outdoor Temperature")+ylab("Residuals")+
  ggtitle("Residuals vs outdoor temperature for males and females")

```

Figure \ref{fig:resit_fit1} shows the residuals of the model estimated with \texttt{lme4}. We can see that there is a different variance each sex.




## Question 3

As shown in \ref{fig:resit_fit1}, we the model should estimate a different variance for each sex. This leads to the following model:

\begin{align}
clo_{i,j,k} &= \mu +\beta(sex_i)+u_i+v_{ij} +\varepsilon_{ijk} \\
u_i\sim& N(0,\sigma^2_u\alpha(sex_i))\\ 
v_{ij}\sim &N(0,\sigma^2_v\alpha(sex_i))\\
\varepsilon_{ijk}\sim& N(0,\sigma^2\alpha(sex_i))
\end{align}

The marginal log-likelihood of this model is almost equal to the previous question: 
\begin{equation}
l_M(y,\beta,\sigma_u,\sigma)\propto -\frac{1}{2}\sum_{i}^{N_{subjId}} \log |\alpha(sex_i) \Sigma^{(i)} |+ (y_i-X_i\beta)^T\left(\alpha(sex_i)\Sigma^{(i)}\right)^{-1}(y_i-X_i\beta) \label{eq:margin_Q3}
\end{equation}

The covariance structure of $\Sigma^{(i)}$ is still the same as in equation (\ref{eq:Sigma_i}). We will however set $\alpha("male")=1$ and $\alpha("female")=a$. 

Furthermore, for our optimization, we estimate the variance parameters and $a$ in the log-domain to ensure that the variance is positive. 



```{r}

nll2<- function(pars,alpha_sex="male"){
  #print(c(pars[1:2],exp(pars[3]+pars[4:6]) ))
  #print(pars)
  # ML estimation:
  mu = pars["mu"]
  sex_male = pars["sex_male"]
  beta=c(mu,sex_male)
  alpha = pars['alpha']
  
  
  ll=0
  for(i in unique(dat$subjId)){
    #print(i)
    df = dat[dat$subjId==i,]
    df$sex
    if(alpha_sex=="male"){
      sex = df$sex_male[1]
    }
    if(alpha_sex=="female"){
      sex = df$sex_female[1]
    }
    
    #print(sex)
    n = nrow(df)
    days = unique(df$day)
    
    if(df$sex[1]=="male"){
      x = cbind(1,rep(1,n))
    }else{
      x = cbind(1,rep(0,n))
    }
    
    sig = diag(n)*exp(sex*alpha+2*pars["sigma"])+exp(sex*alpha+2*pars["sigma_s"])
    for(k in days){
      idx = which(df$day==k)
      sig[idx,idx] = sig[idx,idx]+exp(sex*alpha+2*pars["sigma_sub_day"])
    }
    
    
    y = df$clo
    tmp = dmvn(y,x%*%beta,sig,log=T)
    #tmp = dmvnorm(y,x%*%beta,sig,log=T)
    #print(tmp)
    ll = ll+tmp
  }
  return(-ll)
}
dat$sex_male = as.numeric(as.factor(dat$sex))-1
dat$sex_female = abs(as.numeric(as.factor(dat$sex))-2)
pars=c("mu"=0.5,"sex_male"=-0.1,"alpha"=1,"sigma_sub_day"=0.1,"sigma_s"=0.1,"sigma"=0.1)


# Sanity check that we get the same for both
# Fitting female alpha, alpha_male=1
opt2<-nlminb(pars,nll2,alpha_sex="female")
opt2$objective
out_par = opt2$par
c(out_par[1:2],exp(out_par[3]),exp(out_par[4:6])^2)
exp(out_par[3])*exp(out_par[4:6])^2


# Fitting male alpha, alpha_female=1
opt2<-nlminb(pars,nll2,alpha_sex="male")
opt2$objective
out_par = opt2$par
c(out_par[1:2],exp(out_par[3]),exp(out_par[4:6])^2)
exp(out_par[3])*exp(out_par[4:6])^2

c(out_par[1:2],exp(out_par[3]),exp(out_par[4:6]))
exp(out_par[4:6])
sqrt(exp(out_par[3]))*exp(out_par[4:6])

-nll2(opt2$par)


H = hessian(nll2,opt2$par)





m = cbind(opt2$par,
           opt2$par+qnorm(0.025)*sqrt(diag(solve(H))),
           opt2$par+qnorm(0.975)*sqrt(diag(solve(H))))
m[3:6,] = exp(m[3:6,])

m
xtable::xtable(m,digits=4)

```

The estimated log-likelihood is 1214.929, which means much higher than what we estimated previously. 

\begin{table}[ht]
\centering
\begin{tabular}{|c|ccc|}
  \hline
 & estimate & 2.5\% & 97.5\% \\ 
  \hline
$\mu$ & 0.5926 & 0.5088 & 0.6764 \\ 
  $\beta$ & -0.0849 & -0.1714 & 0.0016 \\ 
  $a$ & 0.0612 & 0.0497 & 0.0752 \\ 
  $\sigma_v$ & 0.2361 & 0.1980 & 0.2814 \\ 
  $\sigma_u$ & 0.1562 & 0.1052 & 0.2318 \\ 
  $\sigma$ & 0.0713 & 0.0666 & 0.0763 \\ 
   \hline
\end{tabular}
\caption{This table shows the estimated parameters and their 95\% (Wald) confidence interval.}\label{tab:q2_3}
\end{table}
Table \ref{tab:q2_3} shows the estimated parameters of our model. If we compare these estimates with table \ref{tab:q2_2}, we can see that all of the variance (standard deviation) parameters have increased. For this model, the estimates for males and females have however been separated. This means we need to compare these estimates and $\sqrt{a}\sigma_x$ to get a proper understanding of the difference. From the increased likelihood (and figure \ref{fig:resit_fit1}) we can see that this newer model with different variances fit the data better. 

## Question 4: Gamma-function for variance scaling.

\textit{A model can be formulated as}

\begin{align}
Y_i | \gamma_i \sim N(\mu,\sigma^2/\gamma_i)\\
\gamma_i \sim(G(1,\phi))
\end{align}
\textit{where $\gamma_i$ follows a gamma distribution with mean 1 and variance $1/\phi$. Show that the marginal distribution can be written as: }
\begin{equation}
f_{Y_i} \sim \frac{1}{\sigma} f_0\left(\frac{y-\mu}{\sigma};2\phi \right)
\end{equation}
\textit{where $f_0$ is the pdf of a student t-distributed random variable with $2\phi$ degrees of freedom.}


First, we write up the pdf for a student t-distribution scaled with $1/\sigma$
\begin{equation}
\frac{1}{\sigma}pdf(\frac{y-\mu}{\sigma},2\phi) = \dfrac{1}{\sigma} \frac{\Gamma(\frac{2\phi+1}{2})}{\sqrt{2\phi \pi}\Gamma(\phi)}\left(1+\frac{(\frac{y-\mu}{\sigma})^2}{2\phi}\right)^{-\frac{2\phi+1}{2}} 
\end{equation}
Here, we recognize this as a shifted and scaled student t-distribution. 

Second, we have to estimate the marginal likelihood:

\begin{equation}
L_M (\mu,\sigma,\phi;y) = \int_{\mathbb{R^{\gamma_i}}} f_{Y|\gamma_i}(y;\gamma_i,\mu,\sigma)f_{\gamma_i}(\gamma_i;\phi)
\end{equation}

We use the gamma distribution with shape and rate parameters $\alpha$ and $\beta$ such that the mean is $\frac{\alpha}{\beta}$ and the variance is $\frac{\alpha}{\beta^2}$:

\begin{align}
L_M (\mu,\sigma,\phi;y) =& \int_{\mathbb{R^{\gamma_i}}} f_{Y|\gamma_i}(y;\gamma_i,\mu,\sigma)f_{\gamma_i}(\gamma_i;\phi)\\
=&\int_0^\infty \frac{\sqrt{\gamma}}{\sqrt{2\pi}\cdot(\sigma)}\exp{\left(-\frac{1}{2}\frac{\gamma(y-\mu)^2}{\sigma^2}\right)} 
\times \frac{\beta^\alpha}{\Gamma(\alpha)}\gamma^{\alpha-1}\exp{(-\gamma\beta)}\\
=& \frac{1}{\sqrt{(2\pi)}\sigma} \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty \sqrt{\gamma}\exp{\left(-\frac{1}{2}\frac{\gamma(y-\mu)^2}{\sigma^2}\right)}
\times\gamma^{\alpha-1}\exp{(-\gamma\beta)} \\
=& \frac{1}{\sqrt{(2\pi)}\sigma} \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty \gamma^{(\alpha+\frac{1}{2})-1}\exp{\left(-\gamma\beta-\frac{1}{2}\frac{\gamma(y-\mu)^2}{\sigma^2}\right)} \\
 =& \frac{1}{\sqrt{(2\pi)}\sigma} \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty \gamma^{(\alpha+\frac{1}{2})-1}\exp{\left(-\gamma \left(\beta+\frac{1}{2}\frac{(y-\mu)^2}{\sigma^2}\right)\right)} \\
 =& \frac{1}{\sqrt{(2\pi)}\sigma} \frac{\beta^\alpha}{\Gamma(\alpha)} \int_0^\infty \gamma^{\hat{\alpha}-1}\exp{\left(-\gamma \hat{\beta}\right)}\\
\end{align}



Now, we recognize the the gamma integral where we set: 

\begin{equation}
\hat{\beta}=\left(\beta+\frac{1}{2}\frac{(y-\mu)^2}{\sigma^2}\right), \hat{\alpha}=\alpha+1/2
\end{equation}


We can now replace the gamma integral:

\begin{align}
L_M (\mu,\sigma,\phi;y) =& \frac{1}{\sqrt{(2\pi)}\sigma} \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{\Gamma(\hat{\alpha})}{\hat{\beta}^{\hat{\alpha}}}\\
=& \frac{1}{\sqrt{(2\pi)}\sigma} \frac{\beta^\alpha}{\Gamma(\alpha)} \frac{\Gamma(\alpha+1/2)}{\left(\beta+\frac{1}{2}\frac{(y-\mu)^2}{\sigma^2}\right)^{\alpha+1/2}}\\
\end{align}


Now, we have to set $\alpha=\phi$ and $\beta =\phi$ to ensure that we get the right mean and variance:

\begin{align}
L_M (\mu,\sigma,\phi;y) =&\frac{1}{\sigma}\frac{\Gamma(\phi+1/2)}{\sqrt{2\pi}\Gamma(\phi)}\phi^\phi
\left(\phi+\frac{(y-\mu)^2}{2\sigma^2}\right)^{-\phi-1/2}\\
=&\frac{1}{\sigma}\frac{\Gamma(\phi+1/2)}{\sqrt{2\pi}\Gamma(\phi)}\phi^\phi
\left(1+\frac{(y-\mu)^2}{2\sigma^2\phi}\right)^{-\phi-1/2}\cdot \left(\frac{1}{\phi}\right)^{-\phi-1/2}\\
=&\frac{1}{\sigma}\frac{\Gamma(\phi+1/2)}{\sqrt{2\pi}\Gamma(\phi)}
\left(1+\frac{(y-\mu)^2}{2\sigma^2\phi}\right)^{-\phi-1/2}\cdot \left(\frac{1}{\phi}\right)^{-1/2}\\
=&\frac{1}{\sigma}\frac{\Gamma(\phi+1/2)}{\sqrt{2\pi \phi}\Gamma(\phi)}
\left(1+\frac{(y-\mu)^2}{2\sigma^2\phi}\right)^{-\phi-1/2}\\
\end{align}

Thus, we have derived the equation.


## Question 5

We are given the following model: 
\begin{align}
clo_{i,j,k}|u_i, v_{ij},\gamma_i \sim& \mathcal{N}(\mu + \beta(sex_i)+u_i + v_{ij}, \sigma^2\alpha(sex_i)/\gamma_i)\\
u_i \sim& \mathcal{N}(0,\sigma_u^2\alpha(sex_i)/\gamma_i)\\
v_{ij} \sim& \mathcal{N}(0, \sigma_v^2\alpha(sex_i)/\gamma_i)\\
\gamma_i \sim& \mathcal{G}(1,\phi)
\end{align}

In Question 4 we found the marginal likelihood of a normal & gamma distribution. This is not exactly the same for Question 5.

We have to marginalize $u_i$ and $v_{ij}$ out so that we have $f_{clo|\gamma_i}$. We do however already have this marginalization from Question 3:

\begin{equation}
f_{clo|\gamma_i} = N(X\beta,V)
\end{equation}

where $V$ is the full covariance matrix of all subjects.

We can therefore replace the univariate normal distribution from Question 4 with this multivariate normal distribution to find our marginal distribution: 

\begin{align}
L_M(y,\beta,\sigma_u,\sigma_v,\sigma,\phi) = & \int_0^{\infty} f_{clo|\gamma_i} (y;\gamma_i,\beta,\sigma,\sigma_u,\sigma_v)f_{\gamma}(\phi)\\
L_M(y,\beta,\sigma_u,\sigma_v,\sigma,\phi) = &\frac{1}{\sqrt{|\alpha(sex_i)V|}}\dfrac{\Gamma(\phi+1/2)}{(2\pi\phi)^{d/2}\Gamma(\phi)}\left( 1 + \dfrac{1}{2\phi}(y-X\beta)^T(\alpha(sex_i)V)^{-1}(y-X\beta)\right)^{-\phi-1/2}  
\end{align}

We once again recognize this as a shifted and scaled multivariate t-distribution. 

Furthermore, we can utilize the grouping structure of $V$ to solve smaller sub-problems of the marginal distribution like in equation (\ref{eq:margin_Q3}).
```{r}

nllQ5<- function(pars,alpha_sex="male"){
  #print(c(pars[1:2],exp(pars[3]+pars[4:6]) ))
  #print(pars)
  # ML estimation:
  mu = pars["mu"]
  sex_male = pars["sex_male"]
  beta=c(mu,sex_male)
  alpha = pars['alpha']
  
  ll=0
  for(i in unique(dat$subjId)){
    #print(i)
    df = dat[dat$subjId==i,]
    df$sex
    if(alpha_sex=="male"){
      sex = df$sex_male[1]
    }
    if(alpha_sex=="female"){
      sex = df$sex_female[1]
    }
    
    n = nrow(df)
    days = unique(df$day)
    
    if(df$sex[1]=="male"){
      x = cbind(1,rep(1,n))
    }else{
      x = cbind(1,rep(0,n))
    }
    
    sig = diag(n)*exp(sex*alpha+2*pars["sigma"])+exp(sex*alpha+2*pars["sigma_s"])
    for(k in days){
      idx = which(df$day==k)
      sig[idx,idx] = sig[idx,idx]+exp(sex*alpha+2*pars["sigma_sub_day"])
    }
    
    y = df$clo
    
    tmp = dmvt(X = y, mu = x%*%beta ,sigma = sig, df = 2*exp(pars["phi"]),log=T)
    

    ll = ll+tmp
  }
  return(-ll)
}
#library(mvtnorm)
library(mvnfast)
pars=c("mu"=0.59,"sex_male"=-0.08,"alpha"=0,"sigma_sub_day"=0,"sigma_s"=0,"sigma"=0,"phi"=1)

# Fitting male alpha, female=1
optQ5<-nlminb(pars,nllQ5,alpha_sex="male")
optQ5$objective
out_par = optQ5$par
parQ5 = out_par
c(out_par[1:2],exp(out_par[3:6]),exp(out_par[7]))
sqrt(exp(out_par[3]))*exp(out_par[4:6])

# # Fitting male alpha, female=1
# optQ5<-nlminb(pars,nllQ5,alpha_sex="female")
# out_par = optQ5$par
# c(out_par[1:2],exp(out_par[3:6]),exp(out_par[7]))
# sqrt(exp(out_par[3]))*exp(out_par[4:6])

library(numDeriv)
H = hessian(nllQ5,optQ5$par)

m = cbind(optQ5$par,
           optQ5$par+qnorm(0.025)*sqrt(diag(solve(H))),
           optQ5$par+qnorm(0.975)*sqrt(diag(solve(H))))
m[3:7,] = exp(m[3:7,])


xtable::xtable(m,digits=4)

```

Again, we optimized the variance parameters in the log-domain to ensure positive variance, and we did set $\alpha("male") = 1$ and $\alpha("female") = a$. The estimated log-likelihood was: 1698.6 therefore being higher than the previous model. The covariance structure of $\Sigma$ is still the same as in equation (\ref{eq:Sigma_i}). 


\begin{table}[H]
\centering
\begin{tabular}{rrrr}
  \hline
 & Estimate & 2.5\% & 97.5\% \\ 
  \hline
$\mu$ & 0.5943 & 0.5458 & 0.6428 \\ 
  $\beta$ & -0.0881 & -0.1430 & -0.0332 \\ 
  $\alpha$ & 0.3490 & 0.1270 & 0.9589 \\ 
  $\sigma_v$ & 0.0875 & 0.0584 & 0.1311 \\ 
  $\sigma_u$ & 0.0865 & 0.0531 & 0.1408 \\ 
  $\sigma$ & 0.0073 & 0.0050 & 0.0105 \\ 
  $\phi$ & 0.3737 & 0.2674 & 0.5222 \\ 
   \hline
\end{tabular}
\caption{This table shows the estimated parameters and their 95\% (Wald) confidence interval for question 5.}\label{tab:coef_q5}
\end{table}

It should be noted that the implementation of $dvmt$ the scaling using the $\Sigma$ matrix are implement implicit in the model and therefore the output does not need to be scaled. 

## Question 6

The following model are given: 

\begin{align}
  clo_{i,j,k}|u_i, v_{ij}, \gamma_i \sim &\mathcal{N}(\mu + \beta(sex_i) + u_i + v_{ij}, \sigma^2\alpha(sex_i)\exp{(-\gamma_i)})\\
  u_i \sim& \mathcal{N}(0,\sigma_u\alpha(sex_i)\exp{(-\gamma_i)})\\
  v_{ij}\sim& \mathcal{N}(0,\sigma_v\alpha(sex_i)\exp{(-\gamma_i)})\\
  \gamma_i \sim& \mathcal{N}(0, \sigma_G^2)
\end{align}

The $\gamma_i$ parameter are now modeled using a normal distribution. This is approximated using Laplace approximation and implement using TMB. The model now contains three random effects namely $u,v,\gamma$. Again, the variance parameters are optimized in the log-domain to ensure positive variance, otherwise one could specify the bounds in the $nlminb$ optimizer. As in the previous tasks we set $\alpha("male") = 1$ and $\alpha("female") = a$.

The estimated log-likelihood was: 1687.9 thereby being smaller than the one estimated modeling the random effect $\gamma$ as Gamma-distributed which yielded an analytic solution for the marginal likelihood. The estimated parameters and 95\% wald confidence intervals can be seen in table \ref{tab:coef_q6}. 

```{r}
library(TMB)
parameters <- list(u=rep(0,length(unique(dat$subjId))),
                   v = rep(0,length(unique(dat$subDay))),
                   gamma = rep(0,length(unique(dat$subjId))),
                   sigma_u = 1,
                   sigma_v = 1,
                   sigma_G = 1,
                   sigma = 1,
                   alpha = 1,
                   beta = 1,
                   mu = 1
)
tmp_df = unique(dat[,c('sex',"sex_male","subjId")])
tmp_df2 = unique(dat[,c('sex',"sex_male","subDay","subjId")])
new_dat = list('clo'=dat$clo,
               'subjId' = dat$subjId,
               'subjId_day' = dat$subDay,
               'sex_male' = dat$sex_male,
               
               'subjId_u'=tmp_df$subjId,
               'alpha_u_sex_male'= tmp_df$sex_male,
               
               'alpha_v_sex_male'= tmp_df2$sex_male,
               'subjId_v' = tmp_df2$subjId
)


compile("test.cpp")
dyn.load(dynlib("test"))
## Define objective function
obj <- MakeADFun(data = new_dat,
                 parameters = parameters,
                 random = c("u","v","gamma"),
                 DLL = "test"
)

opt <- nlminb(obj$par, obj$fn, obj$gr)
opt$objective
parQ6 = opt$par
exp(parQ6[-c(1:2)])
```



\begin{table}[H]
\centering
\begin{tabular}{rrrr}
  \hline
 & Estimate & Lower 2.5\% & Upper 97.5\% \\ 
  \hline
  $\mu$ & 0.5919 & 0.5457 & 0.6381 \\ 
  $\beta$ & -0.0858 & -0.1383 & -0.0332 \\ 
  $\sigma_u$ & 0.3219 & 0.1831 & 0.5659 \\ 
  $\sigma$ & 0.0279 & 0.0179 & 0.0435 \\ 
  $\sigma_v$ & 0.3314 & 0.2043 & 0.5375 \\ 
  $\sigma_G$ & 1.0924 & 0.8867 & 1.3458 \\ 
  $\alpha$ & 0.0514 & 0.0145 & 0.1821 \\ 
   \hline
\end{tabular}
\caption{This table shows the estimated parameters and their 95\% (Wald) confidence interval for question 6.}\label{tab:coef_q6}
\end{table}



## Question 7

When comparing the model results from question 5 and 6 it is clear that the estimated log-likelihood of the model from question 5 yields the highest log-likelihood which indicates that this model is best. It should be noted that the difference of log-likelihood is: 1698.6-1687.9 = 10.7.

Also it should be noted that the Laplace approximation is an approximation and it therefore is expected that this would yield a difference from the optimal likelihood. In table \ref{tab:q27_allmodels}, an overview of the log-likelihood and number of estimated parameters for all fitted Hierarchical models can be seen 

\begin{table}[H]
\centering
\begin{tabular}{lrr}
  \hline
  Model & Log-likelihood & Number of parameters \\ \hline
  Q1: & 643.8 & 4 \\
  Q2: & 943.1 & 5\\
  Q3: & 1214.9 & 6 \\
  Q5: & 1698.6 & 7\\
  Q6: & 1687.9 & 7\\ \hline
\end{tabular}
\caption{The log-likelihood and number of estimated parameters for all estimated Hierarchical models.}\label{tab:q27_allmodels}
\end{table}

The parameters of the resulting models of question 5 and 6 can be seen in table \ref{tab:coef_q5} and \ref{tab:coef_q6} respectively. Here, it should be noted that the fixed parameters are approximately equal, at least to the second digit. The variance parameters for the random effects: $\sigma$, $\sigma_s$, $\sigma_{subDay}$ and $\alpha$ are estimated to different optimal values.

Finally, we estimate the random effects $\gamma_i | clo_i$ for the models estimated in the previous two subsections. The problem is set up using Bayes theorem

\begin{equation}
  f(\gamma_i|clo_i) = f_{y_i, \gamma_i}(u, v,\gamma_i)/f_{clo}(u, v),
\end{equation}

where the marginal distribution $f_{clo}(u, v)$ is considered constant and the likelihood is therefore only optimized over $f_{y_i, \gamma_i}(u, v,\gamma_i)$. This results in the log-likelihood of 1757.0 and 1676.6 for the random effects $\gamma_i | clo_i$ for the models from question 5 and question 6 respectively. 




```{r Q7_opt,cache=TRUE}
#COMPARE WITH PARAMETERS AND gamma|clo_i 
library(mvnfast)
ll_Y_given_gamma_Q6<- function(pars,gamma,alpha_sex="male"){
  #print(c(pars[1:2],exp(pars[3]+pars[4:6]) ))
  #print(pars)
  # ML estimation:
  mu = pars["mu"]
  sex_male = pars["sex_male"]
  beta=c(mu,sex_male)
  alpha = pars['alpha']
  ll=0
  for(i in unique(dat$subjId)){
    #print(i)
    df = dat[dat$subjId==i,]
    df$sex
    if(alpha_sex=="male"){
      sex = df$sex_male[1]
    }
    if(alpha_sex=="female"){
      sex = df$sex_female[1]
    }
    
    #print(sex)
    n = nrow(df)
    days = unique(df$day)
    
    if(df$sex[1]=="male"){
      x = cbind(1,rep(1,n))
    }else{
      x = cbind(1,rep(0,n))
    }
    
    sig = diag(n)*exp(sex*alpha+2*pars["sigma"])+exp(sex*alpha+2*pars["sigma_s"])
    for(k in days){
      idx = which(df$day==k)
      sig[idx,idx] = sig[idx,idx]+exp(sex*alpha+2*pars["sigma_sub_day"])
    }
    
    
    y = df$clo
    tmp = dmvn(y,x%*%beta,sig*exp(-gamma[i+1]),log=T)
    #tmp = dmvnorm(y,x%*%beta,sig,log=T)
    #print(tmp)
    ll = ll+tmp
  }
  return(ll)
}

ll_Y_given_gamma<- function(pars,gamma,alpha_sex="male"){
  #print(c(pars[1:2],exp(pars[3]+pars[4:6]) ))
  #print(pars)
  # ML estimation:
  mu = pars["mu"]
  sex_male = pars["sex_male"]
  beta=c(mu,sex_male)
  alpha = pars['alpha']
  
  
  ll=0
  for(i in unique(dat$subjId)){
    #print(i)
    df = dat[dat$subjId==i,]
    df$sex
    if(alpha_sex=="male"){
      sex = df$sex_male[1]
    }
    if(alpha_sex=="female"){
      sex = df$sex_female[1]
    }
    
    #print(sex)
    n = nrow(df)
    days = unique(df$day)
    
    if(df$sex[1]=="male"){
      x = cbind(1,rep(1,n))
    }else{
      x = cbind(1,rep(0,n))
    }
    
    sig = diag(n)*exp(sex*alpha+2*pars["sigma"])+exp(sex*alpha+2*pars["sigma_s"])
    for(k in days){
      idx = which(df$day==k)
      sig[idx,idx] = sig[idx,idx]+exp(sex*alpha+2*pars["sigma_sub_day"])
    }
    
    
    y = df$clo
    tmp = dmvn(y,x%*%beta,sig/gamma[i+1],log=T)
    #tmp = dmvnorm(y,x%*%beta,sig,log=T)
    #print(tmp)
    ll = ll+tmp
  }
  return(ll)
}

gamma_Q5 <-function(gamma,phi){
  ll = dgamma(gamma,shape=phi,rate=phi,log=T)
  return(ll)
}

gamma_Q6 <-function(gamma,sigma_G){
  dnorm(gamma,mean = 0 ,sd = sigma_G,log=T)
}

get_gamma_Q5<-function(gamma,pars){
  marginal = ll_Y_given_gamma(pars = pars, gamma = gamma)
  gamma_dist = gamma_Q5(gamma = gamma,phi = exp(pars["phi"]))
  ll = marginal+sum(gamma_dist)
  return(-ll)
}

get_gamma_Q6<-function(gamma,pars){
  #print(gamma)
  marginal = ll_Y_given_gamma_Q6(pars = pars, gamma = gamma)
  gamma_dist = gamma_Q6(gamma = gamma,sigma_G = exp(pars["sigma_G"]))
  ll = marginal+sum(gamma_dist)
  return(-ll)
}
gamma = rep(1,length(unique(dat$subjId)))
Q5_gamma = nlminb(gamma,get_gamma_Q5,pars = parQ5,lower = 1e-9)
names(parQ6)[c(2,3,5)] = names(parQ5)[c(2,5,4)]
Q6_gamma = nlminb(gamma,get_gamma_Q6,pars = parQ6)

```

The estimated $\gamma$-values for the two separate models are shown in fig \ref{fig:gamma_vs_gamma} where the $\gamma$ values of the model from question 6 is transformed into the same domain so the comparison can be made. The y-axis of the figure on the right has been log-transformed as well.  

```{r gamma_vs_gamma,fig.cap="This figure shows the estimated $\\gamma$-values for the two values. We transformed the $\\gamma$ values from Question 6 so that they would be in the same domain. Each number correspond to the subject ID and the color corresponds to each sex.",fig.height=5}
library(ggplot2)
new_df = data.frame(ID = 0:46,Q5 = Q5_gamma$par,Q6= exp(Q6_gamma$par))


new_df$sex = unique(dat[,c("subjId","sex")])$sex
p1 = ggplot(new_df,aes(x=Q5,y=Q6,col=sex))+geom_text(aes(label=ID))+
  ggtitle("Q5 gamma vs Q6 exp(gamma)")+xlab("Q5 Gamma")+ylab("Q6 exp(gamma)")
p2 = p1+scale_y_continuous(trans="log")+
  ggtitle("Q5 gamma vs Q6 exp(gamma)\nwith log-transformed y-axis")

gridExtra::grid.arrange(p1,p2,ncol=2)

```

From figure \ref{fig:gamma_vs_gamma}, it should be noted that $\gamma_i|clo_i$ for females appears to be estimated to a numerical larger estimate. It should also be noted, that the gamma estimate of the model from question 6 has a larger spread of estimates compared to that of question 5's model. This is compatible with the higher log-likelihood of $gamma_i|clo_i$ for question 5's model. 

The estimates of all $\gamma$ for the two different models can be seen in \ref{tab:all_gamma}.


```{r}
gammas = data.frame(cbind(Q5_gamma$par,exp(Q6_gamma$par)))
row.names(gammas)=0:46
xtable::xtable(gammas)

```


\begin{table}[H]
\centering
\begin{tabular}{rrr}
  \hline
Subject ID & $\gamma_i$ & $\exp(\gamma_i)$ \\ 
  \hline
0 & 1.15 & 34.22 \\ 
  1 & 0.01 & 0.20 \\ 
  2 & 0.01 & 0.15 \\ 
  3 & 1.19 & 68.37 \\ 
  4 & 0.01 & 0.02 \\ 
  5 & 0.00 & 0.09 \\ 
  6 & 0.58 & 4.66 \\ 
  7 & 1.02 & 2.65 \\ 
  8 & 0.00 & 0.09 \\ 
  9 & 0.01 & 0.16 \\ 
  10 & 0.01 & 0.25 \\ 
  11 & 0.11 & 0.30 \\ 
  12 & 0.91 & 1.61 \\ 
  13 & 0.00 & 0.07 \\ 
  14 & 0.01 & 0.29 \\ 
  15 & 0.09 & 0.27 \\ 
  16 & 1.11 & 38.60 \\ 
  17 & 0.96 & 1.75 \\ 
  18 & 1.12 & 4.57 \\ 
  19 & 0.26 & 0.51 \\ 
  20 & 1.19 & 71.85 \\ 
  21 & 0.74 & 1.06 \\ 
  22 & 1.09 & 3.78 \\ 
  23 & 1.10 & 3.94 \\ 
  24 & 1.13 & 5.04 \\ 
  25 & 1.04 & 2.87 \\ 
  26 & 0.00 & 0.05 \\ 
  27 & 0.79 & 6.80 \\ 
  28 & 1.07 & 3.25 \\ 
  29 & 1.07 & 16.65 \\ 
  30 & 0.00 & 0.11 \\ 
  31 & 1.00 & 2.18 \\ 
  32 & 1.12 & 4.45 \\ 
  33 & 0.08 & 0.23 \\ 
  34 & 0.01 & 0.10 \\ 
  35 & 0.07 & 0.22 \\ 
  36 & 0.01 & 0.12 \\ 
  37 & 0.95 & 1.83 \\ 
  38 & 0.01 & 0.11 \\ 
  39 & 0.66 & 5.27 \\ 
  40 & 0.21 & 0.47 \\ 
  41 & 0.17 & 2.23 \\ 
  42 & 1.11 & 4.37 \\ 
  43 & 0.04 & 0.15 \\ 
  44 & 0.11 & 0.28 \\ 
  45 & 0.11 & 1.53 \\ 
  46 & 0.90 & 8.94 \\ 
   \hline
\end{tabular}
\caption{Table of all gamma values}\label{tab:all_gamma}
\end{table}
