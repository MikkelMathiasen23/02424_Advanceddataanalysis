---
output: html_document
editor_options: 
  chunk_output_type: console
chunk_output_type: console
---

```{r}
source('setup.R')
```

# Problem B

## GLM with subject ID

As we saw from the residuals in Problem A, some of the individuals do produce residuals that are either all positive or all negative. This could indicate that the individuals could have different base preferences of clothing. That is, one individual might be warm most of the time and therefore wear less clothing. Likewise an individual might often be cold and would therefore wear more clothes. This could indicate that our model should have an individual intercept. 


Furthermore, dividing the data into individuals rather than just sex, is a more precise grouping. There is also no individual who is both male and female, so the individuals are nested within the sex grouping. Therefore we can discard the groups depending on sex. 

We cannot estimate any models with both an interaction between both of the indoor/outdoor temperatures and an individual intercept because of a lack of degrees of freedom. Third order interactions are therefore also not possible. We can, however, estimate a model with an interaction between either the indoor or outdoor temperature and the individuals. This will lead to either of the following models:

The first model:
\begin{equation}\label{eq:prob2_full1}
\begin{aligned}
Clo_i=&\beta_0+\beta_1 \cdot t_{InOp,i}+\beta_2 \cdot t_{Out,i}+\beta_3\cdot t_{InOp,i}\cdot t_{Out,i}+\\
& a(individual_i)+b(t_{InOp,i},individual_i)
\end{aligned}
\end{equation}

The second model:

\begin{equation}\label{eq:prob2_full2}
\begin{aligned}
Clo_i=&\beta_0+\beta_1 \cdot t_{InOp,i}+\beta_2 \cdot t_{Out,i}+\beta_3\cdot t_{InOp,i}\cdot t_{Out,i}+\\
& a(individual_i)+b(t_{Out,i},individual_i)
\end{aligned}
\end{equation}
where $\beta_j$ is parameters for the overall intercept and continuous variables, while $a$ and $b$ are functions of either just a categorical variable or a combination of categorical and continuous variables. 

The following table shows the initial models with either an interaction between the indoor/ temperature and the individuals:

```{r}
library(ggplot2)
library(car)
library(xtable)
df2=read.csv(file="./Data/clothingSum.csv")
names(df2)[5]="tIn"
df2$subjId = factor(df2$subjId)
df2$sex=factor(df2$sex)

fitB1 = lm( clo ~ tOut*tIn+subjId*tOut,data = df2)
m1=Anova(fitB1,type=3)
#m1
fitB2 = lm( clo ~ tOut*tIn+subjId*tIn,data = df2)
m2=Anova(fitB2,type=3)
#m2
#xtable(rbind(m1,m2),digits=c(0,3,0,3,3))

```
\begin{table}[H]
\centering
\begin{tabular}{ccccccc}
  \hline
Model & & Sum Sq & Df & F value & Pr($>$F) \\ 
  \hline
&\texttt{(Intercept)} & 0.022 & 1 & 3.386 & 0.073 \\ 
& \texttt{tOut} & 0.023 & 1 & 3.446 & 0.071 \\ 
&  \texttt{tInOp} & 0.017 & 1 & 2.531 & 0.120 \\ 
\textbf{Model 2, eq} $\eqref{eq:prob2_full1}$ &  \texttt{subjId} & 0.480 & 46 & 1.571 & 0.074 \\ 
&  \texttt{tOut:tInOp} & 0.023 & 1 & 3.417 & 0.072 \\ 
&  \texttt{tOut:subjId} & 0.442 & 46 & 1.449 & 0.117 \\ 
&  \texttt{Residuals} & 0.266 & 40 &  &  \\ 
  \hline
&  \texttt{(Intercept)} & 0.002 & 1 & 0.270 & 0.606 \\ 
&  \texttt{tOut} & 0.015 & 1 & 1.695 & 0.200 \\ 
& \texttt{tInOp} & 0.004 & 1 & 0.417 & 0.522 \\ 
\textbf{Model 2, eq} $\eqref{eq:prob2_full2}$&  \texttt{subjId} & 0.370 & 46 & 0.925 & 0.603 \\ 
&  \texttt{tOut:tInOp} & 0.019 & 1 & 2.176 & 0.148 \\ 
&  \texttt{tInOp:subjId} & 0.360 & 46 & 0.899 & 0.639 \\ 
&  \texttt{Residuals} & 0.348 & 40 &  &  \\ 
   \hline
\end{tabular}
\caption{The first rows (until the horizontal line in the middle) significant parameters for a model with an interaction term between the outdoor temperature and the individuals. Below the horizontal line we have a model with interaction term between the indoor temperature and the indivuduals. We can see that none of these interaction terms between the individuals and the temperature are significant. Furthermore we see that those are the first terms to be dropped. We used a Type III partitioning for the ANOVA table.}
\label{tab:prob2_init}
\end{table}

\cref{tab:prob2_init} shows that neither of the interaction terms between the temperatures and the individuals are significant. This means that both of our two initial models from equation \cref{eq:prob2_full1} and \cref{eq:prob2_full2} will be reduced to the same model.

As in part 1, higher order terms are removed before lower order terms. We choose which terms to remove by looking at the Type III partitioning in the ANOVA table. We start by removing the terms with the highest p-value, and keep removing terms until all terms are significant. Each removed term can be seen in table \ref{tab:prob2_deviance}.

```{r}
# Fitting the reduced model
fitB = lm(clo ~ tOut*tIn+subjId,data = df2)
Anova(fitB,type=3)

# Dropping interaction term and refitting
fit2B = lm(clo ~ tOut+tIn+subjId,data = df2)
Anova(fit2B,type=3)

# Dropping tIn and refitting
fit3B = lm( clo ~ tOut+subjId,data = df2)
Anova(fit3B,type=3)
#xtable(anova(fitB1,fitB,fit2B,fit3B))

```

\begin{table}[ht]
\centering
\begin{tabular}{lrrrrrr}
  \hline
 & Res.Df & RSS & Df & Sum of Sq & F & Pr($>$F) \\ 
  \hline
\texttt{clo} $\sim$ \texttt{tOut * tInOp + subjId * tOut} & 40 & 0.27 &  &  &  &  \\ 
  \texttt{clo} $\sim$ \texttt{tOut * tInOp + subjId} & 86 & 0.71 & -46 & -0.44 & 1.45 & 0.1171 \\ 
 \texttt{clo} $\sim$ \texttt{tOut + tInOp + subjId} & 87 & 0.71 & -1 & -0.01 & 1.06 & 0.3092 \\ 
  \texttt{clo} $\sim$ \texttt{tOut + subjId} & 88 & 0.72 & -1 & -0.00 & 0.07 & 0.7978 \\ 
   \hline
\end{tabular}
\caption{Deviance table of each of the models. The first column shows the model in \texttt{R} notation. The last column with the P-value shows the P-value when compared to the model from the line above}
\label{tab:prob2_deviance}
\end{table}

When all the non-significant terms have been removed we end up with the following model

\begin{equation}\label{eq:probB_final}
Clo_i=\beta_0+\beta_1\cdot t_{out,i}+a(individual_i)
\end{equation}

When we estimate the model in \texttt{R}, we do however not get an individual estimate of $\beta_0$. We estimate $\mu=\beta_0+a(individual_1)$ which means the intercept and the first individual parameter (subject ID 11) is estimated as one. This also means that each of the parameters for the other individuals are estimated as $g(individual_i)=a(individual_i)-a(individual_1)$. 

Instead of fitting the above model we can instead fit a model without an overall intercept as they are equivalent:

\begin{equation}\label{eq:probB_final2}
Clo_i=\beta_1\cdot t_{out,i}+a(individual_i)
\end{equation}

This model will ensure an easier interpretation of the subject ID parameters. 

## Visual presentation of the parameters


By investigating the individual subject ID parameters from \cref{eq:probB_final2} we might find an underlying distribution:


```{r par_dist,fig.height=4,fig.cap="The figure to the left shows a histogram of the individual parameters, $g(individual_i)$ (except for the first individual), while the figure to the right shows a QQ plot. Both suggest that the parameters could be gaussian distributed."}
fit3B2 = lm( clo ~ -1+tOut+subjId,data = df2)
#Anova(fit3B,type=3)
#Question 2:
p1=ggplot(data.frame(c()),aes(x=fit3B2$coefficients[-1]))+
  geom_histogram(bins=12)+xlab("Individual parameters")+ylab("Count")+
  ggtitle("Histogram of individual parameters")

p2=ggplot(data.frame(c()),aes(sample=fit3B2$coefficients[-1]))+
  stat_qq()+stat_qq_line()+xlab("Theoretical")+ylab("Sample")+
  ggtitle("QQ plot of individual parameters")

library(gridExtra)
grid.arrange(p1,p2,ncol=2)
```

\cref{fig:par_dist} suggests that the parameters of the individuals could be normally distributed. We do see some heavy tails, but it is close enough for the assumption. Ideally, we would have fitted a mixed random and fixed effect model. 

## Interpretation of the parameters. 

The interpretation of the parameters by using \cref{eq:prob2_full2} instead of \cref{eq:prob2_full1} become very simple. The subject ID estimates the individual intercepts, while $\hat{\beta_1}$ is a parameter that defines the increase or decrease in clothing level as the indoor operating temperature increases or decreases. The uncertainty of $\hat{\beta_1}$ can be found in table \ref{tab:prob2_CI}.
```{r}
# Confidence interval for the fixed effects
m=confint(fit3B2)[1,]
m=cbind(fit3B2$coef[1],t(m))
# CI for sigma function
lm_sigma_conf<-function(fit,CI=0.95){
  # Book page 53: sigma_hat^2~sigma^2*chisq_f/f where f=n-k
  # rearanging: sigma_hat^2*f/chisq_f
  sigma=sigma(fit)
  d_f=fit$df.residual
  conf_sigma2=d_f*sigma^2/qchisq(c((1-CI)/2,1-(1-CI)/2), df = d_f, lower.tail = FALSE)
  conf=sqrt(conf_sigma2)
  names(conf)=paste(c((1-CI)/2,1-(1-CI)/2)*100,"%")
  return(conf)
}
# Combining the results
tmp = lm_sigma_conf(fit3B2)
tmp = cbind(sigma=sigma(fit3B2),t(tmp))
m = rbind(m,tmp)
#xtable(m,digits=4)
```

\begin{table}[H]
\centering
\begin{tabular}{rrrr}
  \hline
 & Estimate & 2.5 \% & 97.5 \% \\ 
  \hline
$\hat{\beta_1}$ & -0.0143 & -0.0203 & -0.0082 \\ 
  $\hat{\sigma}$ & 0.0902 & 0.0786 & 0.1058 \\ 
   \hline
\end{tabular}
\caption{This table shows the estimate and 95 \% confidence interval of the $\beta_1$ parameter from 
$\eqref{eq:prob2_full2}$ and the estimate of $\sigma$. The uncertainty is estimated with a Wald type confidence interval. The estimate confidence interval for $\sigma$ is calculated with the distribution from Theorem 3.5 (page 53) in the book.}\label{tab:prob2_CI}
\end{table}

We could have added each of the intercepts in the table as well, but the table would become quite large and comparing each of the individual intercepts would become difficult. Instead the individual intercepts can be found in figure \ref{fig:par_CI} along with their confidence intervals. From figure \ref{fig:par_CI} it can be seen that most confidence intervals for subject IDs are within each others range. Though it should be noticed that some does not eg. 113 and 123.
```{r par_CI,fig.height=3,fig.cap="This figure shows each parameter with its 95 \\% confidence interval. The uncertainty is estimated with a Wald type confidence interval."}
l = length(fit3B2$coefficients)
std_coef = sqrt(diag(vcov(fit3B2)))[-1]
coef_est = fit3B2$coefficients[-1]

df_coef = data.frame(cbind(coef_est, confint(fit3B2)[-1,]))
library(stringr)
df_coef$names =gsub("subjId","",rownames(df_coef))
df_coef$sex = NaN
for (i in 1:(l-1)){
  df_coef$sex[i] = as.character(df2$sex[df_coef$names[i]==df2$subjId][1])
}
df_coef = df_coef[order(as.numeric(as.character(df_coef$names))),]
df_coef$names = factor(as.numeric(as.character(df_coef$names)))
df_coef$sex = df_coef$sex
#levels(df_coef$names)

ggplot(data = df_coef, aes(x = names, y = coef_est, color = sex)) +
  geom_point()+
  geom_errorbar(aes(ymin=X2.5.., ymax=X97.5..)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  xlab("Subject ID")+ylab("Coefficient estimates")+
  ggtitle("Different intercepts for each Subject ID along with CI")
```



## Prediction & Residual analysis

For this particular model, we run into issues when we want to start making predictions. We can estimate the predictions for our data points and compute the residuals but we cannot make a mean prediction. At least not directly. We can make a prediction for each subject ID. This means we can pick the subjects with the median, 25\% and 75\% quantiles intercepts. We can however not predict a new individual, at least not without assuming some kind of probability distribution.
```{r quantile,fig.height=4,fig.cap="This figure shows the predictions for the individuals who had an intercept corresponding to the 25, 50 and 75\\% quantiles. All plots contain all the data points."}
nam=c()
quant=c(0.25,0.5,0.75)
for(i in quant){
  tmp=names(coef(fit3B2)[coef(fit3B2)%in%quantile(coef(fit3B2)[-1],probs = i,type=3)])
  nam=c(nam,tmp)
}

nam=gsub("subjId","",nam)
nam
t_seq=seq(min(df2$tOut),max(df2$tOut),0.1)

new_data=data.frame(clo=NA,tOut=rep(t_seq,3),subjId=rep(nam,each=length(t_seq)))
conf = predict(fit3B2,new=new_data,interval = 'confidence')
pred = predict(fit3B2, new=new_data,interval = 'prediction')

new_data$subjId=paste("Subject ID:",new_data$subjId)
new_data$subjId=factor(new_data$subjId,levels=paste("Subject ID:",nam))


new_data$clo = conf[,"fit"]
new_data$CI_lwr = conf[,"lwr"]
new_data$CI_upr = conf[,"upr"]
new_data$PI_lwr = pred[,"lwr"]
new_data$PI_upr = pred[,"upr"]
tmp = df2[df2$subjId%in%nam,]
tmp$subjId=as.character(tmp$subjId)
tmp$subjId=paste("Subject ID:",tmp$subjId)
tmp$subjId = factor(tmp$subjId,levels=paste("Subject ID:",nam))
ggplot(data  = new_data, aes(x = tOut, y = clo)) + geom_line(aes(col=subjId),lwd=1)+
  geom_line(aes(x = tOut, y = CI_lwr,col=subjId), lty = 3,lwd=1)+ 
  geom_line(aes(x = tOut, y = CI_upr,col=subjId), lty = 3,lwd=1)+
  geom_line(aes(x = tOut, y = PI_lwr,col=subjId), lty = 2,lwd=1)+
  geom_line(aes(x = tOut, y = PI_upr,col=subjId), lty = 2,lwd=1)+
  facet_wrap(~subjId)+
  geom_point(data = select(df2,-subjId), aes(x = tOut, y = clo),col='black',alpha=0.5)+
  geom_point(data = tmp,aes(x = tOut, y = clo,col=subjId),size=2)+
  xlab("Outdoor Temperature")+ylab("Clothing level")+
  ggtitle(paste("Predictions for subjects with intercept quantile",
                paste0("Q_",quant*100,": ID=",nam,collapse = " ")))

```

\cref{fig:quantile} shows that we can make predictions for each of our individuals. These predictions does not matter for all of the other individuals as each of them have their own intercept. We have to look at the residuals for each data point individually:


```{r prob2_resid,fig.cap="The upper left plot shows the Residuals vs. the fitted values. The plot to the upper right shows a QQ plot. The plot to the lower left is a Scale-Location plot and the final plot to the lower right is a residual vs leverage plot"}
par_opt=par()
par(mfrow=c(2,2),mar=c(2,2,2,2))
plot(fit3B2)
par(mfrow=c(1,1),mar=par_opt$mar)
```

\cref{fig:prob2_resid} shows various plots of the residuals. The plot of the residuals vs the fitted values looks like white noise. This is an indication that our model did indeed catch the underlying pattern in the data. The QQ-plot do show some heavy tails, especially to most positive residuals. It is however still close enough to the theoretical quantiles, that it could be normally distributed. The Scale-Location plot also show an almost flat line, meaning that the variance seem to be constant for all observations. The residuals vs the leverage (Cook's Distance) do not indicate that any of the observation have a high influence that does not correspond with the fitted model. 

In problem A, we had issues with the female observations would contain more noise. Even though we discarded this grouping we can still investigate if this is the case for this model as well:

```{r prob2_day,fig.height=3,fig.cap="This figure shows the residuals vs. the outdoor temperature, where each point is symolised by the observation day. The different colors are the subject IDs."}
df2$residual=resid(fit3B2)
ggplot(df2,aes(x=tOut,y=residual,col=subjId))+
  geom_text(aes(label=day))+geom_path(alpha=0.5)+
  facet_wrap(~sex)+theme(legend.position = "none")+
  ggtitle("Each data point is a number that corresponds to the observation day")+
  xlab("Outdoor Temperature")+ylab("Residuals")
```
\cref{fig:prob2_day} does show that the female group have slightly more variance than the male group. If we fitted a new model with a weight on each group, we would see that the new model would get a lower AIC than our final (part B) model, but the actual predictions/residuals would hardly change enough for us to see the visual difference. This does however still mean that the model with the weights would theoretically be better because of the lower AIC. 

\cref{fig:prob2_day} also shows each data point as a number corresponding to the observation day, while the color indicates different subject IDs. It does not seem like, from a visual perspective, that the observation days have any pattern, which means they probably would not have been useful for modelling. 

